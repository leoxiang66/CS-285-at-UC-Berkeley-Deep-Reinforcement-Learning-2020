{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPO4rHQp+c0MSXB/bQAWbG2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leoxiang66/CS-285-at-UC-Berkeley-Deep-Reinforcement-Learning-2021/blob/main/examples/reinforce_example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GD38llkTna1t",
        "outputId": "cd6dfb20-4d56-47e9-a037-e94970589f97"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/leoxiang66/CS-285-at-UC-Berkeley-Deep-Reinforcement-Learning-2021.git\n",
            "  Cloning https://github.com/leoxiang66/CS-285-at-UC-Berkeley-Deep-Reinforcement-Learning-2021.git to /tmp/pip-req-build-cedzz5kk\n",
            "  Running command git clone -q https://github.com/leoxiang66/CS-285-at-UC-Berkeley-Deep-Reinforcement-Learning-2021.git /tmp/pip-req-build-cedzz5kk\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from RLalgos==0.1.4) (1.12.1+cu113)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from RLalgos==0.1.4) (1.21.6)\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.7/dist-packages (from RLalgos==0.1.4) (0.25.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym->RLalgos==0.1.4) (1.5.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.7/dist-packages (from gym->RLalgos==0.1.4) (4.12.0)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from gym->RLalgos==0.1.4) (0.0.8)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym->RLalgos==0.1.4) (4.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym->RLalgos==0.1.4) (3.8.1)\n",
            "Building wheels for collected packages: RLalgos\n",
            "  Building wheel for RLalgos (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for RLalgos: filename=RLalgos-0.1.4-py3-none-any.whl size=8159 sha256=c718dc95240b940619a5b1d0145af9149965d32d1cc4d8a97f45b66807230aa9\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-zc0boeog/wheels/4c/56/f2/8ddd79049992cb6f5396813f1f0ab97eac7ab2633c7c9eb79d\n",
            "Successfully built RLalgos\n",
            "Installing collected packages: RLalgos\n",
            "  Attempting uninstall: RLalgos\n",
            "    Found existing installation: RLalgos 0.1.3\n",
            "    Uninstalling RLalgos-0.1.3:\n",
            "      Successfully uninstalled RLalgos-0.1.3\n",
            "Successfully installed RLalgos-0.1.4\n"
          ]
        }
      ],
      "source": [
        "!pip install git+https://github.com/leoxiang66/CS-285-at-UC-Berkeley-Deep-Reinforcement-Learning-2021.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from RLalgos import reinforce\n",
        "import gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.distributions import Categorical\n",
        "from collections import deque\n",
        "import numpy as np\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "DW4639KKndRt"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "######################### environment ##########################\n",
        "env_id = \"CartPole-v1\"\n",
        "# Create the env\n",
        "env = gym.make(env_id)\n",
        "\n",
        "# Create the evaluation env\n",
        "eval_env = gym.make(env_id)\n",
        "\n",
        "# Get the state space and action space\n",
        "s_size = env.observation_space.shape[0]\n",
        "a_size = env.action_space.n\n",
        "\n",
        "print(\"_____OBSERVATION SPACE_____ \\n\")\n",
        "print(\"The State Space is: \", s_size)\n",
        "print(\"Sample observation\", env.observation_space.sample()) # Get a random observation\n",
        "\n",
        "print(\"\\n _____ACTION SPACE_____ \\n\")\n",
        "print(\"The Action Space is: \", a_size)\n",
        "print(\"Action Space Sample\", env.action_space.sample()) # Take a random action"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f273pYi2ntGP",
        "outputId": "59a2c268-f891-43ca-b9b1-cf23c3a7d6c9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "_____OBSERVATION SPACE_____ \n",
            "\n",
            "The State Space is:  4\n",
            "Sample observation [-4.1055722e+00  8.9001972e+37  2.1976815e-01 -1.6123223e+38]\n",
            "\n",
            " _____ACTION SPACE_____ \n",
            "\n",
            "The Action Space is:  2\n",
            "Action Space Sample 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/core.py:318: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  \"Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/wrappers/step_api_compatibility.py:40: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  \"Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "######################## policy #################################\n",
        "class Policy(nn.Module):\n",
        "    def __init__(self, s_size, a_size, h_size):\n",
        "        super(Policy, self).__init__()\n",
        "        # Create two fully connected layers\n",
        "        self.fc1 = nn.Linear(s_size, h_size)\n",
        "        self.fc2 = nn.Linear(h_size, a_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Define the forward pass\n",
        "        # state goes to fc1 then we apply ReLU activation function\n",
        "        # fc1 outputs goes to fc2\n",
        "        # We output the softmax\n",
        "\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return F.softmax(x, dim=1)\n",
        "\n",
        "    def act(self, state):\n",
        "        \"\"\"\n",
        "        Given a state, take action\n",
        "        \"\"\"\n",
        "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
        "        probs = self.forward(state).cpu()\n",
        "        m = Categorical(probs)\n",
        "        action = m.sample()\n",
        "        return action.item(), m.log_prob(action)\n"
      ],
      "metadata": {
        "id": "CekD9MI5oHQk"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "########################## training #############################\n",
        "cartpole_hyperparameters = {\n",
        "    \"h_size\": 16,\n",
        "    \"n_training_episodes\": 1000,\n",
        "    \"n_evaluation_episodes\": 10,\n",
        "    \"max_t\": 1000,\n",
        "    \"gamma\": 1.0,\n",
        "    \"lr\": 1e-2,\n",
        "    \"env_id\": env_id,\n",
        "    \"state_space\": s_size,\n",
        "    \"action_space\": a_size,\n",
        "}\n",
        "\n",
        "# Create policy and place it to the device\n",
        "cartpole_policy = Policy(cartpole_hyperparameters[\"state_space\"], cartpole_hyperparameters[\"action_space\"], cartpole_hyperparameters[\"h_size\"]).to(device)\n",
        "cartpole_optimizer = optim.Adam(cartpole_policy.parameters(), lr=cartpole_hyperparameters[\"lr\"])\n",
        "\n",
        "scores = reinforce(cartpole_policy,\n",
        "                   cartpole_optimizer,\n",
        "                   cartpole_hyperparameters[\"n_training_episodes\"],\n",
        "                   cartpole_hyperparameters[\"max_t\"],\n",
        "                   cartpole_hyperparameters[\"gamma\"],\n",
        "                   100,\n",
        "                   env)\n",
        "\n",
        "#################################################################"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wQRf3VStn-mz",
        "outputId": "6a64d5c6-a2cf-4c61-91e2-3e779dd4bbef"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 100\tAverage Score: 22.12\n",
            "Episode 200\tAverage Score: 54.87\n",
            "Episode 300\tAverage Score: 63.88\n",
            "Episode 400\tAverage Score: 70.27\n",
            "Episode 500\tAverage Score: 80.27\n",
            "Episode 600\tAverage Score: 71.05\n",
            "Episode 700\tAverage Score: 74.84\n",
            "Episode 800\tAverage Score: 65.15\n",
            "Episode 900\tAverage Score: 71.18\n",
            "Episode 1000\tAverage Score: 57.10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###################### evaluation ###############################\n",
        "def evaluate_agent(env, max_steps, n_eval_episodes, policy):\n",
        "    \"\"\"\n",
        "    Evaluate the agent for ``n_eval_episodes`` episodes and returns average reward and std of reward.\n",
        "    :param env: The evaluation environment\n",
        "    :param n_eval_episodes: Number of episode to evaluate the agent\n",
        "    :param policy: The Reinforce agent\n",
        "    \"\"\"\n",
        "    episode_rewards = []\n",
        "    for episode in range(n_eval_episodes):\n",
        "        state = env.reset()\n",
        "        step = 0\n",
        "        done = False\n",
        "        total_rewards_ep = 0\n",
        "\n",
        "        for step in range(max_steps):\n",
        "            action, _ = policy.act(state)\n",
        "            new_state, reward, done, info = env.step(action)\n",
        "            total_rewards_ep += reward\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "            state = new_state\n",
        "        episode_rewards.append(total_rewards_ep)\n",
        "    mean_reward = np.mean(episode_rewards)\n",
        "    std_reward = np.std(episode_rewards)\n",
        "\n",
        "    return mean_reward, std_reward\n",
        "\n",
        "evaluate_agent(eval_env,\n",
        "               cartpole_hyperparameters[\"max_t\"],\n",
        "               cartpole_hyperparameters[\"n_evaluation_episodes\"],\n",
        "               cartpole_policy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NYIH59HGoLtl",
        "outputId": "e0c72a90-ca57-4d0b-8bfa-4bb6bdb8250c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(52.8, 14.722771478223793)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    }
  ]
}