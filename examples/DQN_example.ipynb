{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNwpx7dBWjb/fqm97gQTVUC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leoxiang66/CS-285-at-UC-Berkeley-Deep-Reinforcement-Learning-2021/blob/main/examples/DQN_example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Pha4EEYsQ0Uq"
      },
      "outputs": [],
      "source": [
        "import gym, random\n",
        "from datetime import timedelta\n",
        "import numpy as np\n",
        "import os\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import csv"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deep Q Network Definition\n",
        "In this section, we build  Q(s,a;θ)  function approximation. Since the input is composed of 4 scalars, namely:\n",
        "\n",
        "[position of cart, velocity of cart, angle of pole, rotation rate of pole]\n",
        "we build a FCN -> ReLU -> FCN -> ReLU -> FCN neural network. \n",
        "\n",
        "As an exercice, change the architecture of the network:\n",
        "Change the two fully-connected layers from 8 hidden neurons to 16\n",
        "Add a third layer to the network, with no activation function and self.num_actions as the output size"
      ],
      "metadata": {
        "id": "yEmWOmuhRROp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DQN(nn.Module):\n",
        "    def __init__(self, input_shape, num_actions):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.input_shape = input_shape\n",
        "        self.num_actions = num_actions\n",
        "\n",
        "        self.fc1 = nn.Linear(self.input_shape[0], 8)\n",
        "        self.fc2 = nn.Linear(8, 8)\n",
        "        self.fc3 = nn.Linear(8, self.num_actions)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc2(F.relu(self.fc1(x))))\n",
        "        x = self.fc3(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "Hizh2CkVQ8vs"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Safety Checks"
      ],
      "metadata": {
        "id": "BafjKFPoRlVV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Network Architecture\n",
        "As a *safety check*, inspect the resulting network in the next cell. For instance, the total number of trainable parameters should change with the architecture. Check the correctness of `in_features` and `out_features`."
      ],
      "metadata": {
        "id": "S1qDJBxZRqFX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env_id = 'CartPole-v0'\n",
        "env    = gym.make(env_id)\n",
        "network = DQN(env.observation_space.shape, env.action_space.n)\n",
        "\n",
        "print(\"Observation space:\\n\", env.observation_space.shape, \"\\n\")\n",
        "print(\"Network architecture:\\n\", network, \"\\n\")\n",
        "\n",
        "model_parameters = filter(lambda p: p.requires_grad, network.parameters())\n",
        "print(\"Total number of trainable parameters:\\n\", sum([np.prod(p.size()) for p in model_parameters]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xM3yowG_RG_G",
        "outputId": "025908aa-fa50-4750-cdf9-a39fe5d2c86d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Observation space:\n",
            " (4,) \n",
            "\n",
            "Network architecture:\n",
            " DQN(\n",
            "  (fc1): Linear(in_features=4, out_features=8, bias=True)\n",
            "  (fc2): Linear(in_features=8, out_features=8, bias=True)\n",
            "  (fc3): Linear(in_features=8, out_features=2, bias=True)\n",
            ") \n",
            "\n",
            "Total number of trainable parameters:\n",
            " 130\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/envs/registration.py:594: UserWarning: \u001b[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1`.\u001b[0m\n",
            "  f\"The environment {id} is out of date. You should consider \"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/core.py:318: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  \"Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/wrappers/step_api_compatibility.py:40: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  \"Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Experience Replay Memory\n",
        "As usual RL tasks have no pre-generated training sets which they can learn from, in off-policy learning, our agent must keep records of all the state-transitions it encountered so it can **learn from them later**. The memory-buffer used to store this is often referred to as the **Experience Replay Memory**. There are several types and architectures of these memory buffers — but some very common ones are:\n",
        "- the *cyclic memory buffers*: they make sure the agent keeps training over its new behavior rather than things that might no longer be relevant\n",
        "- the *reservoir-sampling-based memory buffers*: they guarantee each state-transition recorded has an even probability to be inserted to the buffer\n",
        "\n",
        "We use a combination of both."
      ],
      "metadata": {
        "id": "gmxbwnfDSQg7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ExperienceReplayMemory:\n",
        "    def __init__(self, capacity):\n",
        "        self.capacity = capacity\n",
        "        self.memory = []\n",
        "\n",
        "    def push(self, transition):\n",
        "        self.memory.append(transition)\n",
        "        # This function needs an `if` statement in order to keep the capacity to its limit. Write it below.\n",
        "        # Hint: `del something` will delete something if something is an array\n",
        "        if len(self.memory) > self.capacity:\n",
        "            del self.memory[0]\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        return random.sample(self.memory, batch_size)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)"
      ],
      "metadata": {
        "id": "IKKAAr4nRyJD"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Agent declaration"
      ],
      "metadata": {
        "id": "586Kbv8ITOyv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Agent(object):\n",
        "    def __init__(self, config, env, log_dir='/tmp/gym'):\n",
        "        self.log_dir = log_dir\n",
        "        self.rewards = []\n",
        "        self.action_log_frequency = config.ACTION_SELECTION_COUNT_FREQUENCY\n",
        "        self.action_selections = [0 for _ in range(env.action_space.n)]\n",
        "    \n",
        "    # Define the DQN networks\n",
        "    def declare_networks(self):\n",
        "        self.model = DQN(self.num_feats, self.num_actions)\n",
        "        # Create `self.target_model` with the same network architecture\n",
        "        self.target_model = DQN(self.num_feats, self.num_actions)\n",
        "\n",
        "    # Define the Replay Memory\n",
        "    def declare_memory(self):\n",
        "        self.memory = ExperienceReplayMemory(self.experience_replay_size)\n",
        "    \n",
        "    # Append the new transition to the Replay Memory\n",
        "    def append_to_replay(self, s, a, r, s_):\n",
        "        self.memory.push((s, a, r, s_))\n",
        "    \n",
        "    # Sample transitions from the Replay Memory\n",
        "    def sample_minibatch(self):\n",
        "        transitions = self.memory.sample(self.batch_size)\n",
        "        batch_state, batch_action, batch_reward, batch_next_state = zip(*transitions)\n",
        "\n",
        "        shape = (-1,)+self.num_feats\n",
        "\n",
        "        batch_state = torch.tensor(batch_state, device=self.device, dtype=torch.float).view(shape)\n",
        "        batch_action = torch.tensor(batch_action, device=self.device, dtype=torch.long).squeeze().view(-1, 1)\n",
        "        batch_reward = torch.tensor(batch_reward, device=self.device, dtype=torch.float).squeeze().view(-1, 1)\n",
        "        \n",
        "        non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch_next_state)), device=self.device, dtype=torch.uint8)\n",
        "        # Sometimes all next states are false\n",
        "        try:\n",
        "            non_final_next_states = torch.tensor([s for s in batch_next_state if s is not None], device=self.device, dtype=torch.float).view(shape)\n",
        "            empty_next_state_values = False\n",
        "        except:\n",
        "            non_final_next_states = None\n",
        "            empty_next_state_values = True\n",
        "\n",
        "        return batch_state, batch_action, batch_reward, non_final_next_states, non_final_mask, empty_next_state_values\n",
        "    \n",
        "    # Sample action\n",
        "    def get_action(self, s, eps=0.1):\n",
        "        with torch.no_grad():\n",
        "            # Epsilon-greedy\n",
        "            if np.random.random() >= eps:\n",
        "                X = torch.tensor([s], device=self.device, dtype=torch.float)\n",
        "                a = self.model(X).max(1)[1].view(1, 1)\n",
        "                return a.item()\n",
        "            else:\n",
        "                return np.random.randint(0, self.num_actions)"
      ],
      "metadata": {
        "id": "AAEMmCP5TJJv"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Learning(Agent):\n",
        "    def __init__(self, env=None, config=None, log_dir='/tmp/gym'):\n",
        "        super().__init__(config=config, env=env, log_dir=log_dir)\n",
        "    \n",
        "    # Compute loss from the Bellman Optimality Equation\n",
        "    def compute_loss(self, batch_vars):\n",
        "        batch_state, batch_action, batch_reward, non_final_next_states, non_final_mask, empty_next_state_values = batch_vars\n",
        "\n",
        "        # Estimate\n",
        "        current_q_values = self.model(batch_state).gather(1, batch_action)\n",
        "        \n",
        "        # Target\n",
        "        with torch.no_grad():\n",
        "            max_next_q_values = torch.zeros(self.batch_size, device=self.device, dtype=torch.float).unsqueeze(dim=1)\n",
        "            if not empty_next_state_values:\n",
        "                max_next_action = self.get_max_next_state_action(non_final_next_states)\n",
        "                max_next_q_values[non_final_mask] = self.target_model(non_final_next_states).gather(1, max_next_action)\n",
        "        # From the equation above, write the value `expected_q_values`.\n",
        "            expected_q_values = batch_reward + self.gamma*max_next_q_values\n",
        "        \n",
        "        # From the equation above, write the value `diff`.\n",
        "        diff = (expected_q_values - current_q_values)\n",
        "        loss = self.MSE(diff)\n",
        "        loss = loss.mean()\n",
        "\n",
        "        return loss\n",
        "\n",
        "    # Update both networks (the agent and the target)\n",
        "    def update(self, s, a, r, s_, sample_idx=0):\n",
        "        self.append_to_replay(s, a, r, s_)\n",
        "        \n",
        "        # When not to update ?\n",
        "        # There is a concise way to write to skip the update, fill in the 2 blanks in the `if` statement below.\n",
        "        # Hint: the sample count should be < the learn_start hyperparameter and respect the update_freq.\n",
        "        # if ... or ...:\n",
        "        if sample_idx < self.learn_start or sample_idx % self.update_freq != 0:\n",
        "            return None\n",
        "\n",
        "        batch_vars = self.sample_minibatch()\n",
        "        loss = self.compute_loss(batch_vars)\n",
        "\n",
        "        # Optimize the model\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        for param in self.model.parameters():\n",
        "            param.grad.data.clamp_(-1, 1)\n",
        "        self.optimizer.step()\n",
        "\n",
        "        self.update_target_model()\n",
        "        self.save_td(loss.item(), sample_idx)\n",
        "\n",
        "    def update_target_model(self):\n",
        "        # Copy weights from model to target_model following `target_net_update_freq`.\n",
        "        self.update_count+=1\n",
        "        if self.update_count % self.target_net_update_freq == 0:\n",
        "            self.target_model.load_state_dict(self.model.state_dict())"
      ],
      "metadata": {
        "id": "SD9unebfToaT"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Model(Learning):\n",
        "    def __init__(self, env=None, config=None, log_dir='/tmp/gym'):\n",
        "        super().__init__(config=config, env=env, log_dir=log_dir)\n",
        "        self.device = config.device\n",
        "\n",
        "        # Hyperparameters\n",
        "        self.gamma = config.GAMMA\n",
        "        self.target_net_update_freq = config.TARGET_NET_UPDATE_FREQ\n",
        "        self.experience_replay_size = config.EXP_REPLAY_SIZE\n",
        "        self.batch_size = config.BATCH_SIZE\n",
        "        self.learn_start = config.LEARN_START\n",
        "        self.update_freq = config.UPDATE_FREQ\n",
        "\n",
        "        # Environment specific parameters\n",
        "        self.num_feats = env.observation_space.shape\n",
        "        self.num_actions = env.action_space.n\n",
        "        self.env = env\n",
        "\n",
        "        self.declare_networks()\n",
        "        self.declare_memory()\n",
        "        self.target_model.load_state_dict(self.model.state_dict())\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=config.LR)\n",
        "        \n",
        "        # Move to correct device\n",
        "        self.model = self.model.to(self.device)\n",
        "        self.target_model.to(self.device)\n",
        "        \n",
        "        self.model.train()\n",
        "        self.target_model.train()\n",
        "        \n",
        "        self.update_count = 0\n",
        "            \n",
        "    def save_td(self, td, tstep):\n",
        "        with open(os.path.join(self.log_dir, 'td.csv'), 'a') as f:\n",
        "            writer = csv.writer(f)\n",
        "            writer.writerow((tstep, td))\n",
        "\n",
        "    def get_max_next_state_action(self, next_states):\n",
        "        return self.target_model(next_states).max(dim=1)[1].view(-1, 1)\n",
        "    \n",
        "    def MSE(self, x):\n",
        "        return 0.5 * x.pow(2)\n",
        "\n",
        "    def save_reward(self, reward):\n",
        "        self.rewards.append(reward)\n",
        "\n",
        "    def save_action(self, action, tstep):\n",
        "        self.action_selections[int(action)] += 1.0/self.action_log_frequency\n",
        "        if (tstep+1) % self.action_log_frequency == 0:\n",
        "            with open(os.path.join(self.log_dir, 'action_log.csv'), 'a') as f:\n",
        "                writer = csv.writer(f)\n",
        "                writer.writerow(list([tstep]+self.action_selections))\n",
        "            self.action_selections = [0 for _ in range(len(self.action_selections))]\n",
        "            \n",
        "    def save_w(self):\n",
        "        if not os.path.exists(\"../saved_agents\"):\n",
        "            os.makedirs(\"../saved_agents\")\n",
        "        torch.save(self.model.state_dict(), '../saved_agents/model.dump')\n",
        "        torch.save(self.optimizer.state_dict(), '../saved_agents/optim.dump')"
      ],
      "metadata": {
        "id": "vFtQeDMmTuBi"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Config"
      ],
      "metadata": {
        "id": "Ya5biGcJUEDx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Config(object):\n",
        "    def __init__(self):\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        \n",
        "        # Main agent variables\n",
        "        self.GAMMA=0.99\n",
        "        self.LR=1e-3\n",
        "        \n",
        "        # Epsilon variables\n",
        "        self.epsilon_start    = 1.0\n",
        "        self.epsilon_final    = 0.01\n",
        "        self.epsilon_decay    = 10000\n",
        "        self.epsilon_by_sample = lambda sample_idx: config.epsilon_final + (config.epsilon_start - config.epsilon_final) * math.exp(-1. * sample_idx / config.epsilon_decay)\n",
        "\n",
        "        # Memory\n",
        "        self.TARGET_NET_UPDATE_FREQ = 1000\n",
        "        self.EXP_REPLAY_SIZE = 10000\n",
        "        self.BATCH_SIZE = 64\n",
        "\n",
        "        # Learning control variables\n",
        "        self.LEARN_START = 1000\n",
        "        self.MAX_SAMPLES = 50000\n",
        "        self.UPDATE_FREQ = 1\n",
        "\n",
        "        # Data logging parameters\n",
        "        self.ACTION_SELECTION_COUNT_FREQUENCY = 1000\n",
        "        \n",
        "        \n",
        "config = Config()"
      ],
      "metadata": {
        "id": "4tQ5CyXwUGXz"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "H2spHCTZULsY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "\n",
        "log_dir = \"/tmp/gym/\"\n",
        "\n",
        "if os.path.exists(log_dir):\n",
        "    os.system(f'rm -rf {log_dir}')\n",
        "os.makedirs(log_dir)\n",
        "\n",
        "\n",
        "env_id = 'CartPole-v0'\n",
        "env    = gym.make(env_id)\n",
        "    \n",
        "model  = Model(env=env, config=config, log_dir=log_dir)\n",
        "\n",
        "episode_reward = 0\n",
        "episode_reward_record = []\n",
        "observation = env.reset()\n",
        "episode = 0\n",
        "\n",
        "for sample_idx in range(1, config.MAX_SAMPLES + 1):\n",
        "    \n",
        "    epsilon = config.epsilon_by_sample(sample_idx)\n",
        "\n",
        "    action = model.get_action(observation, epsilon)\n",
        "    # Log action selection\n",
        "    model.save_action(action, sample_idx)\n",
        "\n",
        "    prev_observation=observation\n",
        "    observation, reward, done, _ = env.step(action)\n",
        "    observation = None if done else observation\n",
        "\n",
        "    model.update(prev_observation, action, reward, observation, sample_idx)\n",
        "    episode_reward += reward\n",
        "\n",
        "    if done:\n",
        "        observation = env.reset()\n",
        "        model.save_reward(episode_reward)\n",
        "        episode_reward_record.append(episode_reward)\n",
        "\n",
        "        episode += 1\n",
        "        if episode % 100 == 0:\n",
        "            print(f'{episode}th episode, mean eposide reward: {np.mean(episode_reward_record)}')\n",
        "        episode_reward = 0\n",
        "\n",
        "        \n",
        "        \n",
        "\n",
        "    \n",
        "\n",
        "        \n",
        "\n",
        "model.save_w()\n",
        "env.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XMLizLMvUI4X",
        "outputId": "c4521c70-d9fb-4713-b8a3-d073997bd244"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/envs/registration.py:594: UserWarning: \u001b[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1`.\u001b[0m\n",
            "  f\"The environment {id} is out of date. You should consider \"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/core.py:318: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  \"Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/wrappers/step_api_compatibility.py:40: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  \"Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:17: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  ../aten/src/ATen/native/IndexingUtils.h:27.)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100th episode, mean eposide reward: 22.4\n",
            "200th episode, mean eposide reward: 34.835\n",
            "300th episode, mean eposide reward: 74.91\n",
            "400th episode, mean eposide reward: 98.35\n",
            "500th episode, mean eposide reward: 98.194\n"
          ]
        }
      ]
    }
  ]
}