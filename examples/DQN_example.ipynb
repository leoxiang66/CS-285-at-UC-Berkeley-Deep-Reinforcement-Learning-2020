{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyN/OKzGr3qX4H/vS9Yb+Hsv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leoxiang66/CS-285-at-UC-Berkeley-Deep-Reinforcement-Learning-2021/blob/main/examples/DQN_example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/leoxiang66/CS-285-at-UC-Berkeley-Deep-Reinforcement-Learning-2021.git"
      ],
      "metadata": {
        "id": "4mC1RSPrb_3q",
        "outputId": "94b5c2c1-c2ee-4dde-9458-b2852fe72131",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/leoxiang66/CS-285-at-UC-Berkeley-Deep-Reinforcement-Learning-2021.git\n",
            "  Cloning https://github.com/leoxiang66/CS-285-at-UC-Berkeley-Deep-Reinforcement-Learning-2021.git to /tmp/pip-req-build-99o0ht95\n",
            "  Running command git clone -q https://github.com/leoxiang66/CS-285-at-UC-Berkeley-Deep-Reinforcement-Learning-2021.git /tmp/pip-req-build-99o0ht95\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from RLalgos==0.2.1) (1.12.1+cu113)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from RLalgos==0.2.1) (1.21.6)\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.7/dist-packages (from RLalgos==0.2.1) (0.25.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym->RLalgos==0.2.1) (1.5.0)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from gym->RLalgos==0.2.1) (0.0.8)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.7/dist-packages (from gym->RLalgos==0.2.1) (4.12.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym->RLalgos==0.2.1) (4.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym->RLalgos==0.2.1) (3.8.1)\n",
            "Building wheels for collected packages: RLalgos\n",
            "  Building wheel for RLalgos (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for RLalgos: filename=RLalgos-0.2.1-py3-none-any.whl size=10833 sha256=385beb54139427db72d8a8ebb06c2dfa812a009c764e2ee8740cc6f74609f3a0\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-dfxah5s5/wheels/4c/56/f2/8ddd79049992cb6f5396813f1f0ab97eac7ab2633c7c9eb79d\n",
            "Successfully built RLalgos\n",
            "Installing collected packages: RLalgos\n",
            "  Attempting uninstall: RLalgos\n",
            "    Found existing installation: RLalgos 0.2.0\n",
            "    Uninstalling RLalgos-0.2.0:\n",
            "      Successfully uninstalled RLalgos-0.2.0\n",
            "Successfully installed RLalgos-0.2.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Pha4EEYsQ0Uq"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import os\n",
        "import torch\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from RLalgos import DQN"
      ],
      "metadata": {
        "id": "-iTvwrNIcLzV"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Config"
      ],
      "metadata": {
        "id": "Ya5biGcJUEDx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Config(object):\n",
        "    def __init__(self):\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        \n",
        "        # Main agent variables\n",
        "        self.GAMMA=0.99\n",
        "        self.LR=1e-3\n",
        "        \n",
        "        # Epsilon variables\n",
        "        self.epsilon_start    = 1.0\n",
        "        self.epsilon_final    = 0.01\n",
        "        self.epsilon_decay    = 10000\n",
        "        self.epsilon_by_sample = lambda sample_idx: config.epsilon_final + (config.epsilon_start - config.epsilon_final) * math.exp(-1. * sample_idx / config.epsilon_decay)\n",
        "\n",
        "        # Memory\n",
        "        self.TARGET_NET_UPDATE_FREQ = 1000\n",
        "        self.EXP_REPLAY_SIZE = 10000\n",
        "        self.BATCH_SIZE = 64\n",
        "\n",
        "        # Learning control variables\n",
        "        self.LEARN_START = 1000\n",
        "        self.MAX_SAMPLES = 50000\n",
        "        self.UPDATE_FREQ = 1\n",
        "\n",
        "        # Data logging parameters\n",
        "        self.ACTION_SELECTION_COUNT_FREQUENCY = 1000\n",
        "        \n",
        "        \n",
        "config = Config()"
      ],
      "metadata": {
        "id": "4tQ5CyXwUGXz"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "H2spHCTZULsY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "log_dir = \"/tmp/gym/\"\n",
        "\n",
        "if os.path.exists(log_dir):\n",
        "    os.system(f'rm -rf {log_dir}')\n",
        "os.makedirs(log_dir)\n",
        "\n",
        "\n",
        "env_id = 'CartPole-v0'\n",
        "env    = gym.make(env_id)\n",
        "    \n",
        "model  = DQN(env=env, config=config, log_dir=log_dir)\n",
        "\n",
        "episode_reward = 0\n",
        "episode_reward_record = []\n",
        "observation = env.reset()\n",
        "episode = 0\n",
        "\n",
        "for sample_idx in range(1, config.MAX_SAMPLES + 1):\n",
        "    \n",
        "    epsilon = config.epsilon_by_sample(sample_idx)\n",
        "\n",
        "    action = model.get_action(observation, epsilon)\n",
        "    # Log action selection\n",
        "    model.save_action(action, sample_idx)\n",
        "\n",
        "    prev_observation=observation\n",
        "    observation, reward, done, _ = env.step(action)\n",
        "    observation = None if done else observation\n",
        "\n",
        "    model.update(prev_observation, action, reward, observation, sample_idx)\n",
        "    episode_reward += reward\n",
        "\n",
        "    if done:\n",
        "        observation = env.reset()\n",
        "        model.save_reward(episode_reward)\n",
        "        episode_reward_record.append(episode_reward)\n",
        "\n",
        "        episode += 1\n",
        "        if episode % 100 == 0:\n",
        "            print(f'{episode}th episode, mean eposide reward: {np.mean(episode_reward_record)}')\n",
        "        episode_reward = 0\n",
        "\n",
        "        \n",
        "        \n",
        "\n",
        "    \n",
        "\n",
        "        \n",
        "\n",
        "model.save_w()\n",
        "env.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XMLizLMvUI4X",
        "outputId": "6d723694-314c-4577-80fa-e6147e98a766"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/envs/registration.py:594: UserWarning: \u001b[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1`.\u001b[0m\n",
            "  f\"The environment {id} is out of date. You should consider \"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/core.py:318: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  \"Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/wrappers/step_api_compatibility.py:40: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  \"Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n",
            "/usr/local/lib/python3.7/dist-packages/RLalgos/DQN.py:98: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n",
            "  X = torch.tensor([s], device=self.device, dtype=torch.float)\n",
            "/usr/local/lib/python3.7/dist-packages/RLalgos/DQN.py:121: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  ../aten/src/ATen/native/IndexingUtils.h:27.)\n",
            "  max_next_q_values[non_final_mask] = self.target_model(non_final_next_states).gather(1, max_next_action)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100th episode, mean eposide reward: 25.05\n",
            "200th episode, mean eposide reward: 33.01\n",
            "300th episode, mean eposide reward: 70.48\n",
            "400th episode, mean eposide reward: 90.6975\n"
          ]
        }
      ]
    }
  ]
}