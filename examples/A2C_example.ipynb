{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMQACk/8VLXjiKFSoi5OWJV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leoxiang66/CS-285-at-UC-Berkeley-Deep-Reinforcement-Learning-2021/blob/main/examples/A2C_example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.distributions import Categorical\n",
        "from collections import deque\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "wUWOoHdKXkXS"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "MOJxamnzXhCF"
      },
      "outputs": [],
      "source": [
        "class ActorNetwork(nn.Module):\n",
        "\n",
        "    def __init__(self, input_size, hidden_size, action_size):\n",
        "        super(ActorNetwork, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.fc3 = nn.Linear(hidden_size, action_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.fc1(x))\n",
        "        out = F.relu(self.fc2(out))\n",
        "        out = F.softmax(self.fc3(out), dim=-1)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ValueNetwork(nn.Module):\n",
        "\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(ValueNetwork, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.fc3 = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.fc1(x))\n",
        "        out = F.relu(self.fc2(out))\n",
        "        out = self.fc3(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "0XbPeI46XoQ2"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_seed(seed):\n",
        "    np.random.seed(seed=seed)\n",
        "    torch.manual_seed(seed=seed)"
      ],
      "metadata": {
        "id": "4mrFNLd9XtsO"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class A2CAgent:\n",
        "\n",
        "    def __init__(self, config):\n",
        "        self.config = config\n",
        "        self.env = gym.make(config['env_id'])\n",
        "        make_seed(config['seed'])\n",
        "        self.env.seed(config['seed'])\n",
        "        # self.monitor_env = Monitor(self.env, \"./gym-results\", force=True, video_callable=lambda episode: True)\n",
        "        self.gamma = config['gamma']\n",
        "        \n",
        "        # Our two networks\n",
        "        self.value_network = ValueNetwork(self.env.observation_space.shape[0], 16, 1)\n",
        "        self.actor_network = ActorNetwork(self.env.observation_space.shape[0], 16, self.env.action_space.n)\n",
        "        \n",
        "        # Their optimizers\n",
        "        self.value_network_optimizer = optim.RMSprop(self.value_network.parameters(), lr=config['value_network']['learning_rate'])\n",
        "        self.actor_network_optimizer = optim.RMSprop(self.actor_network.parameters(), lr=config['actor_network']['learning_rate'])\n",
        "        \n",
        "    # Hint: use it during training_batch\n",
        "    def _returns_advantages(self, rewards, dones, values, next_value):\n",
        "        \"\"\"Returns the cumulative discounted rewards at each time step\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        rewards : array\n",
        "            An array of shape (batch_size,) containing the rewards given by the env\n",
        "        dones : array\n",
        "            An array of shape (batch_size,) containing the done bool indicator given by the env\n",
        "        values : array\n",
        "            An array of shape (batch_size,) containing the values given by the value network\n",
        "        next_value : float\n",
        "            The value of the next state given by the value network\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        returns : array\n",
        "            The cumulative discounted rewards\n",
        "        advantages : array\n",
        "            The advantages\n",
        "        \"\"\"\n",
        "        \n",
        "        returns = np.append(np.zeros_like(rewards), [next_value], axis=0)\n",
        "        \n",
        "        for t in reversed(range(rewards.shape[0])):\n",
        "            returns[t] = rewards[t] + self.gamma * returns[t + 1] * (1 - dones[t])\n",
        "            \n",
        "        returns = returns[:-1]\n",
        "        advantages = returns - values\n",
        "        return returns, advantages\n",
        "\n",
        "    def training_batch(self, epochs, batch_size):\n",
        "        \"\"\"Perform a training by batch\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        epochs : int\n",
        "            Number of epochs\n",
        "        batch_size : int\n",
        "            The size of a batch\n",
        "        \"\"\"\n",
        "        episode_count = 0\n",
        "        actions = np.empty((batch_size,), dtype=np.int)\n",
        "        dones = np.empty((batch_size,), dtype=np.bool)\n",
        "        rewards, values = np.empty((2, batch_size), dtype=np.float)\n",
        "        observations = np.empty((batch_size,) + self.env.observation_space.shape, dtype=np.float)\n",
        "        observation = self.env.reset()\n",
        "        rewards_test = []\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            # Lets collect one batch\n",
        "            for i in range(batch_size):\n",
        "                observations[i] = observation\n",
        "                values[i] = self.value_network(torch.tensor(observation, dtype=torch.float)).detach().numpy()\n",
        "                policy = self.actor_network(torch.tensor(observation, dtype=torch.float))\n",
        "                actions[i] = torch.multinomial(policy, 1).detach().numpy()\n",
        "                observation, rewards[i], dones[i], _ = self.env.step(actions[i])\n",
        "\n",
        "                if dones[i]:\n",
        "                    observation = self.env.reset()\n",
        "\n",
        "            # If our epiosde didn't end on the last step we need to compute the value for the last state\n",
        "            if dones[-1]:\n",
        "                next_value = 0\n",
        "            else:\n",
        "                next_value = self.value_network(torch.tensor(observation, dtype=torch.float)).detach().numpy()[0]\n",
        "            \n",
        "            # Update episode_count\n",
        "            episode_count += sum(dones)\n",
        "\n",
        "            # Compute returns and advantages\n",
        "            returns, advantages = self._returns_advantages(rewards, dones, values, next_value)\n",
        "\n",
        "            # Learning step !\n",
        "            self.optimize_model(observations, actions, returns, advantages)\n",
        "\n",
        "            # Test it every 50 epochs\n",
        "            if epoch % 50 == 0 or epoch == epochs - 1:\n",
        "                rewards_test.append(np.array([self.evaluate() for _ in range(50)]))\n",
        "                print(f'Epoch {epoch}/{epochs}: Mean rewards: {round(rewards_test[-1].mean(), 2)}, Std: {round(rewards_test[-1].std(), 2)}')\n",
        "\n",
        "                # Early stopping\n",
        "                if rewards_test[-1].mean() > 490 and epoch != epochs -1:\n",
        "                    print('Early stopping !')\n",
        "                    break\n",
        "                observation = self.env.reset()\n",
        "                    \n",
        "        # # Plotting\n",
        "        # r = pd.DataFrame((itertools.chain(*(itertools.product([i], rewards_test[i]) for i in range(len(rewards_test))))), columns=['Epoch', 'Reward'])\n",
        "        # sns.lineplot(x=\"Epoch\", y=\"Reward\", data=r, ci='sd');\n",
        "        \n",
        "        # print(f'The trainnig was done over a total of {episode_count} episodes')\n",
        "\n",
        "    def optimize_model(self, observations, actions, returns, advantages):\n",
        "        actions = F.one_hot(torch.tensor(actions), self.env.action_space.n)\n",
        "        returns = torch.tensor(returns[:, None], dtype=torch.float)\n",
        "        advantages = torch.tensor(advantages, dtype=torch.float)\n",
        "        observations = torch.tensor(observations, dtype=torch.float)\n",
        "\n",
        "        # MSE for the values\n",
        "        self.value_network_optimizer.zero_grad()\n",
        "        values = self.value_network(observations)\n",
        "        loss_value = 1 * F.mse_loss(values, returns)\n",
        "        loss_value.backward()\n",
        "        self.value_network_optimizer.step()\n",
        "\n",
        "        # Actor loss\n",
        "        self.actor_network_optimizer.zero_grad()\n",
        "        policies = self.actor_network(observations)\n",
        "        loss_policy = ((actions.float() * policies.log()).sum(-1) * advantages).mean()\n",
        "        loss_entropy = - (policies * policies.log()).sum(-1).mean()\n",
        "        loss_actor = - loss_policy - 0.0001 * loss_entropy\n",
        "        loss_actor.backward()\n",
        "        self.actor_network_optimizer.step()\n",
        "        \n",
        "        return loss_value, loss_actor    \n",
        "\n",
        "    def evaluate(self):\n",
        "        env = self.env\n",
        "        observation = env.reset()\n",
        "        observation = torch.tensor(observation, dtype=torch.float)\n",
        "        reward_episode = 0\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            policy = self.actor_network(observation)\n",
        "            action = torch.multinomial(policy, 1)\n",
        "            observation, reward, done, info = env.step(int(action))\n",
        "            observation = torch.tensor(observation, dtype=torch.float)\n",
        "            reward_episode += reward\n",
        "            \n",
        "        env.close()\n",
        "\n",
        "        return reward_episode"
      ],
      "metadata": {
        "id": "KNy7aJH4XrO1"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Config A2C {display-mode: \"form\", run: \"auto\"}\n",
        "from pprint import pprint\n",
        "env_id = 'CartPole-v1'  #@param [\"CartPole-v1\", \"Acrobot-v1\", \"MountainCar-v0\"]\n",
        "value_learning_rate = 0.001  #@param {type: \"number\"}\n",
        "actor_learning_rate = 0.001  #@param {type: \"number\"}\n",
        "gamma = 1  #@param {type: \"number\"}\n",
        "entropy = 1  #@param {type: \"number\"}\n",
        "seed = 1  #@param {type: \"integer\"}\n",
        "#@markdown ---\n",
        "\n",
        "config_a2c = {\n",
        "    'env_id': env_id,\n",
        "    'gamma': gamma,\n",
        "    'seed': seed,\n",
        "    'value_network': {'learning_rate': value_learning_rate},\n",
        "    'actor_network': {'learning_rate': actor_learning_rate},\n",
        "    'entropy': entropy\n",
        "}\n",
        "\n",
        "print(\"Current config_a2c is:\")\n",
        "pprint(config_a2c)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p8vvjG9KX0rN",
        "outputId": "932c33d0-5af3-46c6-8343-8dde146c3691"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current config_a2c is:\n",
            "{'actor_network': {'learning_rate': 0.001},\n",
            " 'entropy': 1,\n",
            " 'env_id': 'CartPole-v1',\n",
            " 'gamma': 1,\n",
            " 'seed': 1,\n",
            " 'value_network': {'learning_rate': 0.001}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "agent = A2CAgent(config_a2c)\n",
        "rewards = agent.training_batch(1000, 256)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zwbbbYMlYU5j",
        "outputId": "fb4270d2-1c54-40f9-93d9-ec0cb6867c44"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/core.py:318: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  \"Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/wrappers/step_api_compatibility.py:40: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  \"Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/core.py:257: DeprecationWarning: \u001b[33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed)` instead.\u001b[0m\n",
            "  \"Function `env.seed(seed)` is marked as deprecated and will be removed in the future. \"\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:62: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:63: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:64: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0/1000: Mean rewards: 22.64, Std: 11.85\n",
            "Epoch 50/1000: Mean rewards: 37.08, Std: 20.54\n",
            "Epoch 100/1000: Mean rewards: 64.68, Std: 41.18\n",
            "Epoch 150/1000: Mean rewards: 82.0, Std: 44.5\n",
            "Epoch 200/1000: Mean rewards: 182.94, Std: 87.27\n",
            "Epoch 250/1000: Mean rewards: 218.14, Std: 85.63\n",
            "Epoch 300/1000: Mean rewards: 291.76, Std: 127.34\n",
            "Epoch 350/1000: Mean rewards: 348.24, Std: 133.85\n",
            "Epoch 400/1000: Mean rewards: 404.06, Std: 119.66\n",
            "Epoch 450/1000: Mean rewards: 376.48, Std: 102.97\n",
            "Epoch 500/1000: Mean rewards: 463.82, Std: 78.56\n",
            "Epoch 550/1000: Mean rewards: 409.14, Std: 96.66\n",
            "Epoch 600/1000: Mean rewards: 474.84, Std: 56.25\n",
            "Epoch 650/1000: Mean rewards: 457.72, Std: 87.65\n",
            "Epoch 700/1000: Mean rewards: 429.46, Std: 99.18\n",
            "Epoch 750/1000: Mean rewards: 489.18, Std: 47.07\n",
            "Epoch 800/1000: Mean rewards: 428.58, Std: 90.44\n",
            "Epoch 850/1000: Mean rewards: 478.78, Std: 69.87\n",
            "Epoch 900/1000: Mean rewards: 476.58, Std: 78.6\n",
            "Epoch 950/1000: Mean rewards: 476.48, Std: 75.73\n",
            "Epoch 999/1000: Mean rewards: 496.24, Std: 24.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "agent.evaluate()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rdHQXBf_YYfu",
        "outputId": "811eccc1-8483-4a8b-eb04-70ddba7f76f3"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "500.0"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    }
  ]
}